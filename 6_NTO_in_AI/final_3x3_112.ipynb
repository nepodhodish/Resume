{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHciHyC2bDYS"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "!cp /content/drive/MyDrive/latest/train.tar.gz /content/\n",
        "!tar -xzvf /content/train.tar.gz\n",
        "!cp /content/drive/MyDrive/latest/train.csv /content/\n",
        "\n",
        "!python -m pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSIib9XGY6Fs",
        "outputId": "733dfea4-ec68-43ba-d7aa-6ec15b9f42be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-fho2qpjl\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-fho2qpjl\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a9b1bf5920416aaeaec965c25dd9e8f98c864f16\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (4.64.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (0.14.1+cu116)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy->clip==1.0) (0.2.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision->clip==1.0) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision->clip==1.0) (2.25.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->clip==1.0) (8.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (4.0.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: more_itertools in /usr/local/lib/python3.8/dist-packages (9.0.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install transformers\n",
        "!pip install more_itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAGDCFZ6aOCA"
      },
      "outputs": [],
      "source": [
        "import clip\n",
        "import os\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as nnf\n",
        "import sys\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from tqdm import tqdm, trange\n",
        "import PIL.Image\n",
        "from IPython.display import Image\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85FkKUgCaQLz"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, sizes, bias=True, act=nn.Tanh):\n",
        "        super(MLP, self).__init__()\n",
        "        layers = []\n",
        "        for i in range(len(sizes) -1):\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n",
        "            if i < len(sizes) - 2:\n",
        "                layers.append(act())\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class ClipCaptionModel(nn.Module):\n",
        "\n",
        "    def __init__(self, prefix_length, prefix_size: int = 512):\n",
        "        super(ClipCaptionModel, self).__init__()\n",
        "        self.prefix_length = prefix_length\n",
        "\n",
        "        self.gpt = GPT2LMHeadModel.from_pretrained('sberbank-ai/rugpt3small_based_on_gpt2')\n",
        "\n",
        "        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
        "\n",
        "        if prefix_length > 10:  # not enough memory\n",
        "            self.clip_project = nn.Linear(10,47)#prefix_size, self.gpt_embedding_size * prefix_length)\n",
        "        else:\n",
        "            self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2, self.gpt_embedding_size * prefix_length))\n",
        "\n",
        "    #@functools.lru_cache #FIXME\n",
        "    def get_dummy_token(self, batch_size, device):\n",
        "        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n",
        "\n",
        "    def forward(self, tokens, prefix, mask, labels):\n",
        "        embedding_text = self.gpt.transformer.wte(tokens)\n",
        "        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
        "        #print(embedding_text.size()) #torch.Size([5, 67, 768])\n",
        "        #print(prefix_projections.size()) #torch.Size([5, 1, 768])\n",
        "        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n",
        "        if labels is not None:\n",
        "            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n",
        "            labels = torch.cat((dummy_token, tokens), dim=1)\n",
        "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ClipCaptionPrefix(ClipCaptionModel):\n",
        "\n",
        "    def parameters(self, recurse = True):\n",
        "        return self.clip_project.parameters()\n",
        "\n",
        "    def train(self, mode = True):\n",
        "        super(ClipCaptionPrefix, self).train(mode)\n",
        "        self.gpt.eval()\n",
        "        return self"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S21C4pkyarFN"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRjeRPvia6uv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "data = []\n",
        "for video_name, question, answer in zip(df_train.video_name, df_train.question, df_train.answer):\n",
        "    name = f'videos/{video_name}.mp4'\n",
        "    if os.path.exists(name):\n",
        "        data += [(name,f'Q: {question} A: {answer}')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bjj0uB4gccCS"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "import PIL\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import transformers\n",
        "import more_itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "from dataclasses import dataclass, field\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "import cv2\n",
        "import clip\n",
        "from PIL import Image\n",
        "import pickle\n",
        "from tqdm.contrib import tzip\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def image_grid(imgs, rows, cols):\n",
        "    pils = imgs\n",
        "\n",
        "    assert len(imgs) == rows*cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid\n",
        "\n",
        "def read_video(path, transform=None, frames_num=4, window=30):\n",
        "    frames = []\n",
        "    cap = cv2.VideoCapture(path)\n",
        "\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    N = length//(frames_num)\n",
        "    #print(length)\n",
        "    #counter =\n",
        "\n",
        "    current_frame = 1\n",
        "    for i in range(length):\n",
        "\n",
        "        #frameId = int(round(cap.get(current_frame)))\n",
        "        #print(current_frame)\n",
        "        ret, frame = cap.read(current_frame)\n",
        "\n",
        "\n",
        "        if ret and i==current_frame and len(frames)<frames_num:\n",
        "            size = 168, 168\n",
        "            frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "            frame.thumbnail(size, Image.ANTIALIAS)\n",
        "\n",
        "            frames.append(frame)\n",
        "            current_frame += N\n",
        "\n",
        "\n",
        "        #print(current_frame)\n",
        "        #cap.set(cv2.CAP_PROP_POS_FRAMES, current_frame)\n",
        "\n",
        "\n",
        "    cap.release()\n",
        "    #print(frames)\n",
        "    return frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uR5W2TTccdv"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda')\n",
        "clip_model_type = \"ViT-L/14@336px\"\n",
        "\n",
        "out_path = f\"/content/drive/MyDrive/latest/Features_train_full.pkl\"\n",
        "video_path =  'videos'\n",
        "\n",
        "\n",
        "clip_model, preprocess = clip.load(clip_model_type, device=device, jit=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fBnjfERcjlp"
      },
      "outputs": [],
      "source": [
        "clip_model.to(device)\n",
        "None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#prefix = 20\n",
        "#batch = 15"
      ],
      "metadata": {
        "id": "DLMzIF7T6u_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388,
          "referenced_widgets": [
            "668fa2e6d525479a9f7f97e7ab5983a5",
            "a155c8076f104e74a5f1207979833a3d",
            "fdd89c6d9e0d439d83d0da15814ba391",
            "51c2c394c46847d188aa7f01d7553184",
            "11f743891d714b5ea200d3265857aca6",
            "9482fe805bc740e787e776a65c0066c6",
            "9452a4f0b23c4028bfa1156072239c3e",
            "ca3d539a70364596abf667c4465a255c",
            "83d31aba6c604f1a996ccaa9bcfd8f08",
            "58d946ec8a0e4b5197aff33bbb650bac",
            "73cd0a2d07aa49869e2a10e46b2e72e0"
          ]
        },
        "id": "kwOK5wascju4",
        "outputId": "af2dc832-a0a2-4881-9935-1e37776bb42b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/18993 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "668fa2e6d525479a9f7f97e7ab5983a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-cce81845a2ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mvideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m#print(len(video))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-602b4dd8034f>\u001b[0m in \u001b[0;36mread_video\u001b[0;34m(path, transform, frames_num, window)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m#frameId = int(round(cap.get(current_frame)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m#print(current_frame)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "all_embeddings = []\n",
        "all_captions = []\n",
        "i = 0\n",
        "\n",
        "for video_name, question, answer in tzip(df_train.video_name, df_train.question, df_train.answer):\n",
        "\n",
        "\n",
        "    name = f'{video_path}/{video_name}.mp4'\n",
        "\n",
        "    text = f'Q: {question} A: {answer}'\n",
        "    #print(name)\n",
        "    if os.path.exists(name):\n",
        "\n",
        "        video = read_video(path = name, frames_num=4)\n",
        "        if len(video)>1:\n",
        "            #print(len(video))\n",
        "            image = image_grid(video,2,2)\n",
        "\n",
        "            image = preprocess(image).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                prefix = clip_model.encode_image(image).cpu()\n",
        "            #d[\"clip_embedding\"] = i\n",
        "            all_embeddings.append(prefix)\n",
        "            all_captions.append(text)\n",
        "\n",
        "with open(out_path, 'wb') as f:\n",
        "    pickle.dump({\"clip_embedding\": torch.cat(all_embeddings, dim=0), \"captions\": all_captions}, f)\n",
        "\n",
        "print('Done')\n",
        "print(\"%0d embeddings saved \" % len(all_embeddings))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uF07hEqcltu"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import io\n",
        "\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torchvision\n",
        "import transformers\n",
        "import more_itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "from dataclasses import dataclass, field\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import pickle\n",
        "import sys\n",
        "from tqdm.contrib import tzip\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as nnf\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import sys\n",
        "import argparse\n",
        "import json\n",
        "from typing import Tuple, Optional, Union\n",
        "from torch.cuda.amp import autocast\n",
        "\n",
        "from transformers.optimization import Adafactor, AdafactorSchedule\n",
        "import torch\n",
        "\n",
        "from torch.utils.checkpoint import checkpoint_sequential\n",
        "\n",
        "class ClipCocoDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_path: str,  prefix_length= 50, gpt2_type = \"sberbank-ai/rugpt3small_based_on_gpt2\",\n",
        "                 normalize_prefix=False):\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
        "        self.prefix_length = prefix_length\n",
        "        self.normalize_prefix = normalize_prefix\n",
        "        with open(data_path, 'rb') as f:\n",
        "            all_data = pickle.load(f)\n",
        "        print(\"Data size is %0d\" % len(all_data[\"clip_embedding\"]))\n",
        "        sys.stdout.flush()\n",
        "        self.prefixes = all_data[\"clip_embedding\"]\n",
        "        captions_raw = all_data[\"captions\"]\n",
        "\n",
        "        #self.image_ids = [caption[\"image_id\"] for caption in captions_raw]\n",
        "\n",
        "        self.captions = captions_raw\n",
        "\n",
        "\n",
        "        self.captions_tokens = []\n",
        "        self.caption2embedding = []\n",
        "        max_seq_len = 0\n",
        "        i=0\n",
        "        for caption in tqdm(captions_raw):\n",
        "                self.captions_tokens.append(torch.tensor(self.tokenizer.encode(caption), dtype=torch.int64))\n",
        "                self.caption2embedding.append(self.prefixes[i])\n",
        "                i+=1\n",
        "                max_seq_len = max(max_seq_len, self.captions_tokens[-1].shape[0])\n",
        "            # self.max_seq_len = max_seq_len\n",
        "        #del self.captions_tokens\n",
        "        #del self.caption2embedding\n",
        "        #gc.collect()\n",
        "        #with open(f\"{data_path[:-4]}_tokens.pkl\", 'wb') as f:\n",
        "        #        pickle.dump([self.captions_tokens, self.caption2embedding, max_seq_len], f)\n",
        "\n",
        "\n",
        "\n",
        "        all_len = torch.tensor([len(self.captions_tokens[i]) for i in range(len(self))]).float()\n",
        "        self.max_seq_len = min(int(all_len.mean() + all_len.std() * 10), int(all_len.max()))\n",
        "\n",
        "    def pad_tokens(self, item: int):\n",
        "        tokens = self.captions_tokens[item]\n",
        "        padding = self.max_seq_len - tokens.shape[0]\n",
        "        if padding > 0:\n",
        "            tokens = torch.cat((tokens, torch.zeros(padding, dtype=torch.int64) - 1))\n",
        "            self.captions_tokens[item] = tokens\n",
        "        elif padding < 0:\n",
        "            tokens = tokens[:self.max_seq_len]\n",
        "            self.captions_tokens[item] = tokens\n",
        "        mask = tokens.ge(0)  # mask is zero where we out of sequence\n",
        "        tokens[~mask] = 0\n",
        "        mask = mask.float()\n",
        "        mask = torch.cat((torch.ones(self.prefix_length), mask), dim=0)  # adding prefix mask\n",
        "        return tokens, mask\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.captions_tokens)\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        tokens, mask = self.pad_tokens(item)\n",
        "        prefix = self.prefixes[item]\n",
        "        if self.normalize_prefix:\n",
        "            prefix = prefix.float()\n",
        "            prefix = prefix / prefix.norm(2, -1)\n",
        "        return tokens, mask, prefix\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6d-G11Bc-Lb",
        "outputId": "9f45fb5f-fd4a-42b6-ca1f-00865619c501"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data size is 18993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 18993/18993 [00:08<00:00, 2247.01it/s]\n"
          ]
        }
      ],
      "source": [
        "dataset = ClipCocoDataset('/content/drive/MyDrive/latest/Features_3x3_112_L.pkl', prefix_length=20, normalize_prefix=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3N3MQ7afj8m"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n",
        "        super(MLP, self).__init__()\n",
        "        layers = []\n",
        "        for i in range(len(sizes) - 1):\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n",
        "            if i < len(sizes) - 2:\n",
        "                layers.append(act())\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    #@autocast()\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "def freeze(\n",
        "    model,\n",
        "    freeze_emb=False,\n",
        "    freeze_ln=False,\n",
        "    freeze_attn=True,\n",
        "    freeze_ff=True,\n",
        "    freeze_other=True,\n",
        "):\n",
        "\n",
        "    for name, p in model.named_parameters():\n",
        "    # freeze all parameters except the layernorm and positional embeddings\n",
        "\n",
        "\n",
        "\n",
        "        name = name.lower()\n",
        "        if 'ln' in name or 'norm' in name:\n",
        "            p.requires_grad = not freeze_ln\n",
        "        elif 'embeddings' in name:\n",
        "            p.requires_grad = not freeze_emb\n",
        "        elif 'mlp' in name:\n",
        "            p.requires_grad = not freeze_ff\n",
        "        elif 'attn' in name:\n",
        "            p.requires_grad = not freeze_attn\n",
        "        else:\n",
        "            p.requires_grad = not freeze_other\n",
        "\n",
        "    return model\n",
        "\n",
        "class ClipCaptionModel(nn.Module):\n",
        "    def __init__(self, backbone, prefix_length: int, prefix_size: int = 512):\n",
        "          super(ClipCaptionModel, self).__init__()\n",
        "          self.prefix_length = prefix_length\n",
        "          \"\"\"\n",
        "          ru gpts shit\n",
        "\n",
        "          \"\"\"\n",
        "          self.gpt = GPT2LMHeadModel.from_pretrained(backbone)\n",
        "          #self.gpt = freeze(self.gpt)\n",
        "          self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
        "          self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2,\n",
        "                                  self.gpt_embedding_size * prefix_length))\n",
        "\n",
        "    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:\n",
        "        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n",
        "\n",
        "    # @autocast()\n",
        "    def forward(self, tokens: torch.Tensor, prefix: torch.Tensor, mask: Optional[torch.Tensor] = None,\n",
        "                labels: Optional[torch.Tensor] = None):\n",
        "\n",
        "        embedding_text = self.gpt.transformer.wte(tokens)\n",
        "        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
        "\n",
        "        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n",
        "        if labels is not None:\n",
        "            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n",
        "            labels = torch.cat((dummy_token, tokens), dim=1)\n",
        "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ClipCaptionPrefix(ClipCaptionModel):\n",
        "\n",
        "    def parameters(self, recurse: bool = True):\n",
        "        return self.clip_project.parameters()\n",
        "\n",
        "    def train(self, mode: bool = True):\n",
        "        super(ClipCaptionPrefix, self).train(mode)\n",
        "        self.gpt.eval()\n",
        "        return self\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train(dataset, model: ClipCaptionModel, args,\n",
        "          warmup_steps: int = 5000, output_dir: str = \".\", output_prefix: str = \"\"):\n",
        "\n",
        "    device = torch.device('cuda')# xm.xla_device()\n",
        "    #\n",
        "    batch_size = args.bs\n",
        "    epochs = args.epochs\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    model = freeze(model)\n",
        "    model.train()\n",
        "    optimizer = AdamW(model.parameters(), lr=args.lr,betas=(0.9, 0.995))\n",
        "    #optimizer = bnb.optim.Adam8bit(model.parameters(), lr=0.001, betas=(0.9, 0.995))\n",
        "    #optimizer = SM3(model.parameters(),lr=args.lr)\n",
        "    #Adafactor(model.parameters(),scale_parameter=True, relative_step=True, warmup_init=True, lr=None)\n",
        "\n",
        "    train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=epochs * len(train_dataloader)\n",
        "    )\n",
        "    #AdafactorSchedule(optimizer)#num_training_steps=epochs * len(train_dataloader\n",
        "    #save_config(args)\n",
        "    #print\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\">>> Training epoch {epoch}\")\n",
        "        sys.stdout.flush()\n",
        "        progress = tqdm(total=len(train_dataloader), desc=output_prefix)\n",
        "        step=0\n",
        "        for idx, (tokens, mask, prefix) in enumerate(train_dataloader):\n",
        "            model.zero_grad()\n",
        "            step+=1\n",
        "            tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.float32)\n",
        "\n",
        "            outputs = model(tokens, prefix, mask)\n",
        "            logits = outputs.logits[:, dataset.prefix_length - 1: -1]\n",
        "\n",
        "            loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), tokens.flatten(), ignore_index=0)\n",
        "\n",
        "            segments = 2\n",
        "\n",
        "\n",
        "            #out = checkpoint_sequential(modules, segments, input_var)\n",
        "\n",
        "            # backpropagate\n",
        "            loss.backward()\n",
        "\n",
        "            #optimizer.zero_grad()\n",
        "            progress.set_postfix({\"loss\": loss.item()})\n",
        "\n",
        "            clipping_value = 0.5 # arbitrary value of your choosing\n",
        "            torch.nn.utils.clip_grad_norm(model.parameters(), clipping_value)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            progress.update()\n",
        "\n",
        "\n",
        "            del tokens\n",
        "            del mask\n",
        "            del prefix\n",
        "            torch.clear_autocast_cache()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            if (idx + 1) % 7000 == 0:\n",
        "                torch.save(\n",
        "                    model.state_dict(),\n",
        "\n",
        "                    os.path.join(output_dir, f\"{output_prefix}_latest.pt\"),\n",
        "                )\n",
        "        progress.close()\n",
        "        if epoch % args.save_every ==0:\n",
        "            torch.save(\n",
        "                model.state_dict(),\n",
        "                os.path.join(output_dir, f\"{output_prefix}-{epoch:03d}-{np.mean(losses):.4f}.pt\"),\n",
        "            )\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "class Args():\n",
        "    def __init__(self):\n",
        "        self.backbone = 'sberbank-ai/rugpt3small_based_on_gpt2'\n",
        "        self.data = '/content/drive/MyDrive/latest/Features_train_full_ru.pkl'\n",
        "        self.out_dir = '/content/drive/MyDrive/latest/checkpoints_larger'\n",
        "        self.prefix = 'prefix_1'\n",
        "        self.epochs = 10\n",
        "        self.save_every = 1\n",
        "        self.prefix_length = 15\n",
        "        self.bs = 15\n",
        "        self.only_prefix = False\n",
        "        self.lr = 5e-5\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = Args()\n",
        "    prefix_length = args.prefix_length\n",
        "    dataset = ClipCocoDataset(args.data, prefix_length)\n",
        "\n",
        "    #model_path = 'prefix_1-003.pt'\n",
        "    model = ClipCaptionModel(backbone = 'sberbank-ai/rugpt3small_based_on_gpt2', prefix_length = 15)\n",
        "    # model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
        "    print(\"Train both prefix and GPT\")\n",
        "    sys.stdout.flush()\n",
        "    train(dataset, model, args, output_dir=args.out_dir, output_prefix=args.prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "TqnMOi63Hkbs",
        "outputId": "8ea432b3-1fc1-4398-c189-a768f037a9d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data size is 18993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 18993/18993 [00:08<00:00, 2292.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train both prefix and GPT\n",
            ">>> Training epoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "prefix_1:   0%|          | 0/1266 [00:00<?, ?it/s, loss=6.85]<ipython-input-26-3e3a2c07a5f3>:145: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), clipping_value)\n",
            "prefix_1:  14%|█▍        | 177/1266 [00:26<02:50,  6.40it/s, loss=5.72]"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-3e3a2c07a5f3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train both prefix and GPT\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-3e3a2c07a5f3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, model, args, warmup_steps, output_dir, output_prefix)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1254\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ema_dn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ema_dt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlock_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1257\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic_miniters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m                     \u001b[0;31m# If no `miniters` was specified, adjust automatically to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mrefresh\u001b[0;34m(self, nolock, lock_args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnolock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(self, msg, pos)\u001b[0m\n\u001b[1;32m   1507\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1509\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1510\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1511\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mprint_status\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mprint_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0mlen_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisp_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0mfp_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m             \u001b[0mlast_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mfp_write\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfp_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             \u001b[0mfp_flush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mlast_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/utils.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# request flush on the background thread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m             \u001b[0;31m# wait for flush to actually get through, if we can.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0;31m# waiting across threads during import can cause deadlocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;31m# wake event thread (message content is ignored)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    616\u001b[0m                 )\n\u001b[1;32m    617\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m     def send_multipart(\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxwaqhsMHmM5"
      },
      "outputs": [],
      "source": [
        "import clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBVI7ejooCtR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqNobiKVGTVi"
      },
      "outputs": [],
      "source": [
        "device = 'cuda'\n",
        "clip_model, preprocess = clip.load(\"ViT-L/14@336px\", device=device, jit=False)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('sberbank-ai/rugpt3small_based_on_gpt2')\n",
        "prefix_length= 50\n",
        "model_path = 'prefix_1-007.pt'\n",
        "model = ClipCaptionModel(backbone = 'gpt2', prefix_length = 50)\n",
        "model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
        "model.to(device)\n",
        "None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnGs2uMtH3IW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cl1Kh03qhq3-"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "import PIL\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import transformers\n",
        "import more_itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "#from tqdm import tqdm\n",
        "from dataclasses import dataclass, field\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "import cv2\n",
        "from PIL import Image\n",
        "def image_grid(imgs, rows, cols):\n",
        "    pils = imgs\n",
        "\n",
        "    assert len(imgs) == rows*cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid\n",
        "def read_video(path, transform=None, frames_num=9, window=30):\n",
        "    frames = []\n",
        "    cap = cv2.VideoCapture(path)\n",
        "\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    N = length//(frames_num)\n",
        "    #print(length)\n",
        "    #counter =\n",
        "\n",
        "    current_frame = 1\n",
        "    for i in range(length):\n",
        "\n",
        "        #frameId = int(round(cap.get(current_frame)))\n",
        "        #print(current_frame)\n",
        "        ret, frame = cap.read(current_frame)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        if ret and i==current_frame and len(frames)<frames_num:\n",
        "            size = 193, 193\n",
        "            frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "            frame.thumbnail(size, Image.ANTIALIAS)\n",
        "\n",
        "            frames.append(frame)\n",
        "            current_frame += N\n",
        "\n",
        "\n",
        "        #print(current_frame)\n",
        "        #cap.set(cv2.CAP_PROP_POS_FRAMES, current_frame)\n",
        "\n",
        "\n",
        "    cap.release()\n",
        "    #print(frames)\n",
        "    return frames\n",
        "\n",
        "\n",
        "\n",
        "def filter_ngrams(output_text):\n",
        "    a_pos = output_text.find(' A:')\n",
        "    sec_a_pos = output_text.find(' A:', a_pos + 1)\n",
        "\n",
        "    return output_text[:sec_a_pos]\n",
        "\n",
        "def generate2(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        tokens=None,\n",
        "        prompt='',\n",
        "        embed=None,\n",
        "        entry_count=1,\n",
        "        entry_length=67,  # maximum number of words\n",
        "        top_p=0.98,\n",
        "        temperature=1.,\n",
        "        stop_token = '.',\n",
        "):\n",
        "    model.eval()\n",
        "    generated_num = 0\n",
        "    generated_list = []\n",
        "    stop_token_index = tokenizer.encode(stop_token)[0]\n",
        "    filter_value = -float(\"Inf\")\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for entry_idx in range(entry_count):\n",
        "            if not tokens:\n",
        "                    tokens = torch.tensor(tokenizer.encode(prompt))\n",
        "                    #print('tokens',tokens)\n",
        "                    tokens = tokens.unsqueeze(0).to(device)\n",
        "\n",
        "            emb_tokens = model.gpt.transformer.wte(tokens)\n",
        "\n",
        "            if embed is not None:\n",
        "                generated = torch.cat((embed, emb_tokens), dim=1)\n",
        "            else:\n",
        "                generated = emb_tokens\n",
        "\n",
        "            for i in range(entry_length):\n",
        "\n",
        "                outputs = model.gpt(inputs_embeds=generated)\n",
        "                logits = outputs.logits\n",
        "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "                cumulative_probs = torch.cumsum(nnf.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
        "                                                    ..., :-1\n",
        "                                                    ].clone()\n",
        "                sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "                logits[:, indices_to_remove] = filter_value\n",
        "                #\n",
        "                top_k = 2000\n",
        "                top_p = 0.98\n",
        "                #print(logits)\n",
        "                #next_token = transformers.top_k_top_p_filtering(logits.to(torch.int64).unsqueeze(0), top_k=top_k, top_p=top_p)\n",
        "                next_token = torch.argmax(logits, -1).unsqueeze(0)\n",
        "                next_token_embed = model.gpt.transformer.wte(next_token)\n",
        "\n",
        "                if tokens is None:\n",
        "                    tokens = next_token\n",
        "                else:\n",
        "                    tokens = torch.cat((tokens, next_token), dim=1)\n",
        "                generated = torch.cat((generated, next_token_embed), dim=1)\n",
        "\n",
        "                if stop_token_index == next_token.item():\n",
        "                    break\n",
        "\n",
        "            output_list = list(tokens.squeeze().cpu().numpy())\n",
        "\n",
        "            output_text = tokenizer.decode(output_list)\n",
        "            output_text = filter_ngrams(output_text)\n",
        "            generated_list.append(output_text)\n",
        "\n",
        "    return generated_list[0]\n",
        "#from tqdm import tqdm, trange\n",
        "\n",
        "\n",
        "def _to_caption(pil_image,prompt=''):\n",
        "    device = 'cuda:0'\n",
        "    image = preprocess(pil_image).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "\n",
        "        prefix = clip_model.encode_image(image).to(device, dtype=torch.float32)\n",
        "        prefix_embed = model.clip_project(prefix).reshape(1, prefix_length, -1)\n",
        "        if prompt:\n",
        "            generated_text_prefix = generate2(model, tokenizer, prompt=prompt, embed=prefix_embed)\n",
        "        else:\n",
        "            generated_text_prefix = generate2(model, tokenizer, embed=prefix_embed)\n",
        "    return generated_text_prefix.replace('\\n',' ').replace('\\xa0','')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijbsQSPJFINb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
