{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHciHyC2bDYS"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "!cp /content/drive/MyDrive/latest/train.tar.gz /content/\n",
        "!tar -xzvf /content/train.tar.gz\n",
        "!cp /content/drive/MyDrive/latest/train.csv /content/\n",
        "\n",
        "!python -m pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSIib9XGY6Fs",
        "outputId": "733dfea4-ec68-43ba-d7aa-6ec15b9f42be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-fho2qpjl\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-fho2qpjl\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a9b1bf5920416aaeaec965c25dd9e8f98c864f16\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (4.64.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (0.14.1+cu116)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy->clip==1.0) (0.2.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision->clip==1.0) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision->clip==1.0) (2.25.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->clip==1.0) (8.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (4.0.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: more_itertools in /usr/local/lib/python3.8/dist-packages (9.0.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install transformers\n",
        "!pip install more_itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAGDCFZ6aOCA"
      },
      "outputs": [],
      "source": [
        "import clip\n",
        "import os\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as nnf\n",
        "import sys\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from tqdm import tqdm, trange\n",
        "import PIL.Image\n",
        "from IPython.display import Image\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85FkKUgCaQLz"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, sizes, bias=True, act=nn.Tanh):\n",
        "        super(MLP, self).__init__()\n",
        "        layers = []\n",
        "        for i in range(len(sizes) -1):\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n",
        "            if i < len(sizes) - 2:\n",
        "                layers.append(act())\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class ClipCaptionModel(nn.Module):\n",
        "\n",
        "    def __init__(self, prefix_length, prefix_size: int = 512):\n",
        "        super(ClipCaptionModel, self).__init__()\n",
        "        self.prefix_length = prefix_length\n",
        "\n",
        "        self.gpt = GPT2LMHeadModel.from_pretrained('sberbank-ai/rugpt3small_based_on_gpt2')\n",
        "\n",
        "        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
        "\n",
        "        if prefix_length > 10:  # not enough memory\n",
        "            self.clip_project = nn.Linear(10,47)#prefix_size, self.gpt_embedding_size * prefix_length)\n",
        "        else:\n",
        "            self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2, self.gpt_embedding_size * prefix_length))\n",
        "\n",
        "    #@functools.lru_cache #FIXME\n",
        "    def get_dummy_token(self, batch_size, device):\n",
        "        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n",
        "\n",
        "    def forward(self, tokens, prefix, mask, labels):\n",
        "        embedding_text = self.gpt.transformer.wte(tokens)\n",
        "        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
        "        #print(embedding_text.size()) #torch.Size([5, 67, 768])\n",
        "        #print(prefix_projections.size()) #torch.Size([5, 1, 768])\n",
        "        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n",
        "        if labels is not None:\n",
        "            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n",
        "            labels = torch.cat((dummy_token, tokens), dim=1)\n",
        "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ClipCaptionPrefix(ClipCaptionModel):\n",
        "\n",
        "    def parameters(self, recurse = True):\n",
        "        return self.clip_project.parameters()\n",
        "\n",
        "    def train(self, mode = True):\n",
        "        super(ClipCaptionPrefix, self).train(mode)\n",
        "        self.gpt.eval()\n",
        "        return self"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S21C4pkyarFN"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRjeRPvia6uv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "data = []\n",
        "for video_name, question, answer in zip(df_train.video_name, df_train.question, df_train.answer):\n",
        "    name = f'videos/{video_name}.mp4'\n",
        "    if os.path.exists(name):\n",
        "        data += [(name,f'Q: {question} A: {answer}')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bjj0uB4gccCS"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "import PIL\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import transformers\n",
        "import more_itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "from dataclasses import dataclass, field\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "import cv2\n",
        "import clip\n",
        "from PIL import Image\n",
        "import pickle\n",
        "from tqdm.contrib import tzip\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def image_grid(imgs, rows, cols):\n",
        "    pils = imgs\n",
        "\n",
        "    assert len(imgs) == rows*cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid\n",
        "\n",
        "def read_video(path, transform=None, frames_num=4, window=30):\n",
        "    frames = []\n",
        "    cap = cv2.VideoCapture(path)\n",
        "\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    N = length//(frames_num)\n",
        "    #print(length)\n",
        "    #counter =\n",
        "\n",
        "    current_frame = 1\n",
        "    for i in range(length):\n",
        "\n",
        "        #frameId = int(round(cap.get(current_frame)))\n",
        "        #print(current_frame)\n",
        "        ret, frame = cap.read(current_frame)\n",
        "\n",
        "\n",
        "        if ret and i==current_frame and len(frames)<frames_num:\n",
        "            size = 168, 168\n",
        "            frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "            frame.thumbnail(size, Image.ANTIALIAS)\n",
        "\n",
        "            frames.append(frame)\n",
        "            current_frame += N\n",
        "\n",
        "\n",
        "        #print(current_frame)\n",
        "        #cap.set(cv2.CAP_PROP_POS_FRAMES, current_frame)\n",
        "\n",
        "\n",
        "    cap.release()\n",
        "    #print(frames)\n",
        "    return frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uR5W2TTccdv"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda')\n",
        "clip_model_type = \"ViT-L/14@336px\"\n",
        "\n",
        "out_path = f\"/content/drive/MyDrive/latest/Features_train_full.pkl\"\n",
        "video_path =  'videos'\n",
        "\n",
        "\n",
        "clip_model, preprocess = clip.load(clip_model_type, device=device, jit=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fBnjfERcjlp"
      },
      "outputs": [],
      "source": [
        "clip_model.to(device)\n",
        "None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#prefix = 20\n",
        "#batch = 15"
      ],
      "metadata": {
        "id": "DLMzIF7T6u_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388,
          "referenced_widgets": [
            "668fa2e6d525479a9f7f97e7ab5983a5",
            "a155c8076f104e74a5f1207979833a3d",
            "fdd89c6d9e0d439d83d0da15814ba391",
            "51c2c394c46847d188aa7f01d7553184",
            "11f743891d714b5ea200d3265857aca6",
            "9482fe805bc740e787e776a65c0066c6",
            "9452a4f0b23c4028bfa1156072239c3e",
            "ca3d539a70364596abf667c4465a255c",
            "83d31aba6c604f1a996ccaa9bcfd8f08",
            "58d946ec8a0e4b5197aff33bbb650bac",
            "73cd0a2d07aa49869e2a10e46b2e72e0"
          ]
        },
        "id": "kwOK5wascju4",
        "outputId": "af2dc832-a0a2-4881-9935-1e37776bb42b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/18993 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "668fa2e6d525479a9f7f97e7ab5983a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-cce81845a2ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mvideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m#print(len(video))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-602b4dd8034f>\u001b[0m in \u001b[0;36mread_video\u001b[0;34m(path, transform, frames_num, window)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m#frameId = int(round(cap.get(current_frame)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m#print(current_frame)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "all_embeddings = []\n",
        "all_captions = []\n",
        "i = 0\n",
        "\n",
        "for video_name, question, answer in tzip(df_train.video_name, df_train.question, df_train.answer):\n",
        "\n",
        "\n",
        "    name = f'{video_path}/{video_name}.mp4'\n",
        "\n",
        "    text = f'Q: {question} A: {answer}'\n",
        "    #print(name)\n",
        "    if os.path.exists(name):\n",
        "\n",
        "        video = read_video(path = name, frames_num=4)\n",
        "        if len(video)>1:\n",
        "            #print(len(video))\n",
        "            image = image_grid(video,2,2)\n",
        "\n",
        "            image = preprocess(image).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                prefix = clip_model.encode_image(image).cpu()\n",
        "            #d[\"clip_embedding\"] = i\n",
        "            all_embeddings.append(prefix)\n",
        "            all_captions.append(text)\n",
        "\n",
        "with open(out_path, 'wb') as f:\n",
        "    pickle.dump({\"clip_embedding\": torch.cat(all_embeddings, dim=0), \"captions\": all_captions}, f)\n",
        "\n",
        "print('Done')\n",
        "print(\"%0d embeddings saved \" % len(all_embeddings))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uF07hEqcltu"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import io\n",
        "\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torchvision\n",
        "import transformers\n",
        "import more_itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "from dataclasses import dataclass, field\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import pickle\n",
        "import sys\n",
        "from tqdm.contrib import tzip\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as nnf\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import sys\n",
        "import argparse\n",
        "import json\n",
        "from typing import Tuple, Optional, Union\n",
        "from torch.cuda.amp import autocast\n",
        "\n",
        "from transformers.optimization import Adafactor, AdafactorSchedule\n",
        "import torch\n",
        "\n",
        "from torch.utils.checkpoint import checkpoint_sequential\n",
        "\n",
        "class ClipCocoDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_path: str,  prefix_length= 50, gpt2_type = \"sberbank-ai/rugpt3small_based_on_gpt2\",\n",
        "                 normalize_prefix=False):\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
        "        self.prefix_length = prefix_length\n",
        "        self.normalize_prefix = normalize_prefix\n",
        "        with open(data_path, 'rb') as f:\n",
        "            all_data = pickle.load(f)\n",
        "        print(\"Data size is %0d\" % len(all_data[\"clip_embedding\"]))\n",
        "        sys.stdout.flush()\n",
        "        self.prefixes = all_data[\"clip_embedding\"]\n",
        "        captions_raw = all_data[\"captions\"]\n",
        "\n",
        "        #self.image_ids = [caption[\"image_id\"] for caption in captions_raw]\n",
        "\n",
        "        self.captions = captions_raw\n",
        "\n",
        "\n",
        "        self.captions_tokens = []\n",
        "        self.caption2embedding = []\n",
        "        max_seq_len = 0\n",
        "        i=0\n",
        "        for caption in tqdm(captions_raw):\n",
        "                self.captions_tokens.append(torch.tensor(self.tokenizer.encode(caption), dtype=torch.int64))\n",
        "                self.caption2embedding.append(self.prefixes[i])\n",
        "                i+=1\n",
        "                max_seq_len = max(max_seq_len, self.captions_tokens[-1].shape[0])\n",
        "            # self.max_seq_len = max_seq_len\n",
        "        #del self.captions_tokens\n",
        "        #del self.caption2embedding\n",
        "        #gc.collect()\n",
        "        #with open(f\"{data_path[:-4]}_tokens.pkl\", 'wb') as f:\n",
        "        #        pickle.dump([self.captions_tokens, self.caption2embedding, max_seq_len], f)\n",
        "\n",
        "\n",
        "\n",
        "        all_len = torch.tensor([len(self.captions_tokens[i]) for i in range(len(self))]).float()\n",
        "        self.max_seq_len = min(int(all_len.mean() + all_len.std() * 10), int(all_len.max()))\n",
        "\n",
        "    def pad_tokens(self, item: int):\n",
        "        tokens = self.captions_tokens[item]\n",
        "        padding = self.max_seq_len - tokens.shape[0]\n",
        "        if padding > 0:\n",
        "            tokens = torch.cat((tokens, torch.zeros(padding, dtype=torch.int64) - 1))\n",
        "            self.captions_tokens[item] = tokens\n",
        "        elif padding < 0:\n",
        "            tokens = tokens[:self.max_seq_len]\n",
        "            self.captions_tokens[item] = tokens\n",
        "        mask = tokens.ge(0)  # mask is zero where we out of sequence\n",
        "        tokens[~mask] = 0\n",
        "        mask = mask.float()\n",
        "        mask = torch.cat((torch.ones(self.prefix_length), mask), dim=0)  # adding prefix mask\n",
        "        return tokens, mask\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.captions_tokens)\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        tokens, mask = self.pad_tokens(item)\n",
        "        prefix = self.prefixes[item]\n",
        "        if self.normalize_prefix:\n",
        "            prefix = prefix.float()\n",
        "            prefix = prefix / prefix.norm(2, -1)\n",
        "        return tokens, mask, prefix\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6d-G11Bc-Lb",
        "outputId": "9f45fb5f-fd4a-42b6-ca1f-00865619c501"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data size is 18993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 18993/18993 [00:08<00:00, 2247.01it/s]\n"
          ]
        }
      ],
      "source": [
        "dataset = ClipCocoDataset('/content/drive/MyDrive/latest/Features_3x3_112_L.pkl', prefix_length=20, normalize_prefix=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3N3MQ7afj8m"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n",
        "        super(MLP, self).__init__()\n",
        "        layers = []\n",
        "        for i in range(len(sizes) - 1):\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n",
        "            if i < len(sizes) - 2:\n",
        "                layers.append(act())\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    #@autocast()\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "def freeze(\n",
        "    model,\n",
        "    freeze_emb=False,\n",
        "    freeze_ln=False,\n",
        "    freeze_attn=True,\n",
        "    freeze_ff=True,\n",
        "    freeze_other=True,\n",
        "):\n",
        "\n",
        "    for name, p in model.named_parameters():\n",
        "    # freeze all parameters except the layernorm and positional embeddings\n",
        "\n",
        "\n",
        "\n",
        "        name = name.lower()\n",
        "        if 'ln' in name or 'norm' in name:\n",
        "            p.requires_grad = not freeze_ln\n",
        "        elif 'embeddings' in name:\n",
        "            p.requires_grad = not freeze_emb\n",
        "        elif 'mlp' in name:\n",
        "            p.requires_grad = not freeze_ff\n",
        "        elif 'attn' in name:\n",
        "            p.requires_grad = not freeze_attn\n",
        "        else:\n",
        "            p.requires_grad = not freeze_other\n",
        "\n",
        "    return model\n",
        "\n",
        "class ClipCaptionModel(nn.Module):\n",
        "    def __init__(self, backbone, prefix_length: int, prefix_size: int = 512):\n",
        "          super(ClipCaptionModel, self).__init__()\n",
        "          self.prefix_length = prefix_length\n",
        "          \"\"\"\n",
        "          ru gpts shit\n",
        "\n",
        "          \"\"\"\n",
        "          self.gpt = GPT2LMHeadModel.from_pretrained(backbone)\n",
        "          #self.gpt = freeze(self.gpt)\n",
        "          self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
        "          self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2,\n",
        "                                  self.gpt_embedding_size * prefix_length))\n",
        "\n",
        "    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:\n",
        "        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n",
        "\n",
        "    # @autocast()\n",
        "    def forward(self, tokens: torch.Tensor, prefix: torch.Tensor, mask: Optional[torch.Tensor] = None,\n",
        "                labels: Optional[torch.Tensor] = None):\n",
        "\n",
        "        embedding_text = self.gpt.transformer.wte(tokens)\n",
        "        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
        "\n",
        "        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n",
        "        if labels is not None:\n",
        "            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n",
        "            labels = torch.cat((dummy_token, tokens), dim=1)\n",
        "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ClipCaptionPrefix(ClipCaptionModel):\n",
        "\n",
        "    def parameters(self, recurse: bool = True):\n",
        "        return self.clip_project.parameters()\n",
        "\n",
        "    def train(self, mode: bool = True):\n",
        "        super(ClipCaptionPrefix, self).train(mode)\n",
        "        self.gpt.eval()\n",
        "        return self\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train(dataset, model: ClipCaptionModel, args,\n",
        "          warmup_steps: int = 5000, output_dir: str = \".\", output_prefix: str = \"\"):\n",
        "\n",
        "    device = torch.device('cuda')# xm.xla_device()\n",
        "    #\n",
        "    batch_size = args.bs\n",
        "    epochs = args.epochs\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    model = freeze(model)\n",
        "    model.train()\n",
        "    optimizer = AdamW(model.parameters(), lr=args.lr,betas=(0.9, 0.995))\n",
        "    #optimizer = bnb.optim.Adam8bit(model.parameters(), lr=0.001, betas=(0.9, 0.995))\n",
        "    #optimizer = SM3(model.parameters(),lr=args.lr)\n",
        "    #Adafactor(model.parameters(),scale_parameter=True, relative_step=True, warmup_init=True, lr=None)\n",
        "\n",
        "    train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=epochs * len(train_dataloader)\n",
        "    )\n",
        "    #AdafactorSchedule(optimizer)#num_training_steps=epochs * len(train_dataloader\n",
        "    #save_config(args)\n",
        "    #print\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\">>> Training epoch {epoch}\")\n",
        "        sys.stdout.flush()\n",
        "        progress = tqdm(total=len(train_dataloader), desc=output_prefix)\n",
        "        step=0\n",
        "        for idx, (tokens, mask, prefix) in enumerate(train_dataloader):\n",
        "            model.zero_grad()\n",
        "            step+=1\n",
        "            tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.float32)\n",
        "\n",
        "            outputs = model(tokens, prefix, mask)\n",
        "            logits = outputs.logits[:, dataset.prefix_length - 1: -1]\n",
        "\n",
        "            loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), tokens.flatten(), ignore_index=0)\n",
        "\n",
        "            segments = 2\n",
        "\n",
        "\n",
        "            #out = checkpoint_sequential(modules, segments, input_var)\n",
        "\n",
        "            # backpropagate\n",
        "            loss.backward()\n",
        "\n",
        "            #optimizer.zero_grad()\n",
        "            progress.set_postfix({\"loss\": loss.item()})\n",
        "\n",
        "            clipping_value = 0.5 # arbitrary value of your choosing\n",
        "            torch.nn.utils.clip_grad_norm(model.parameters(), clipping_value)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            progress.update()\n",
        "\n",
        "\n",
        "            del tokens\n",
        "            del mask\n",
        "            del prefix\n",
        "            torch.clear_autocast_cache()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            if (idx + 1) % 7000 == 0:\n",
        "                torch.save(\n",
        "                    model.state_dict(),\n",
        "\n",
        "                    os.path.join(output_dir, f\"{output_prefix}_latest.pt\"),\n",
        "                )\n",
        "        progress.close()\n",
        "        if epoch % args.save_every ==0:\n",
        "            torch.save(\n",
        "                model.state_dict(),\n",
        "                os.path.join(output_dir, f\"{output_prefix}-{epoch:03d}-{np.mean(losses):.4f}.pt\"),\n",
        "            )\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "class Args():\n",
        "    def __init__(self):\n",
        "        self.backbone = 'sberbank-ai/rugpt3small_based_on_gpt2'\n",
        "        self.data = '/content/drive/MyDrive/latest/Features_train_full_ru.pkl'\n",
        "        self.out_dir = '/content/drive/MyDrive/latest/checkpoints_larger'\n",
        "        self.prefix = 'prefix_1'\n",
        "        self.epochs = 10\n",
        "        self.save_every = 1\n",
        "        self.prefix_length = 15\n",
        "        self.bs = 15\n",
        "        self.only_prefix = False\n",
        "        self.lr = 5e-5\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = Args()\n",
        "    prefix_length = args.prefix_length\n",
        "    dataset = ClipCocoDataset(args.data, prefix_length)\n",
        "\n",
        "    #model_path = 'prefix_1-003.pt'\n",
        "    model = ClipCaptionModel(backbone = 'sberbank-ai/rugpt3small_based_on_gpt2', prefix_length = 15)\n",
        "    # model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
        "    print(\"Train both prefix and GPT\")\n",
        "    sys.stdout.flush()\n",
        "    train(dataset, model, args, output_dir=args.out_dir, output_prefix=args.prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "TqnMOi63Hkbs",
        "outputId": "8ea432b3-1fc1-4398-c189-a768f037a9d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data size is 18993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 18993/18993 [00:08<00:00, 2292.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train both prefix and GPT\n",
            ">>> Training epoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "prefix_1:   0%|          | 0/1266 [00:00<?, ?it/s, loss=6.85]<ipython-input-26-3e3a2c07a5f3>:145: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), clipping_value)\n",
            "prefix_1:  14%|█▍        | 177/1266 [00:26<02:50,  6.40it/s, loss=5.72]"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-3e3a2c07a5f3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train both prefix and GPT\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-3e3a2c07a5f3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, model, args, warmup_steps, output_dir, output_prefix)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1254\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ema_dn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ema_dt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlock_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1257\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic_miniters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m                     \u001b[0;31m# If no `miniters` was specified, adjust automatically to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mrefresh\u001b[0;34m(self, nolock, lock_args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnolock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(self, msg, pos)\u001b[0m\n\u001b[1;32m   1507\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1509\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1510\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1511\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mprint_status\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mprint_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0mlen_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisp_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0mfp_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m             \u001b[0mlast_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mfp_write\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfp_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             \u001b[0mfp_flush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mlast_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/utils.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# request flush on the background thread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m             \u001b[0;31m# wait for flush to actually get through, if we can.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0;31m# waiting across threads during import can cause deadlocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;31m# wake event thread (message content is ignored)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    616\u001b[0m                 )\n\u001b[1;32m    617\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m     def send_multipart(\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxwaqhsMHmM5"
      },
      "outputs": [],
      "source": [
        "import clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBVI7ejooCtR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqNobiKVGTVi"
      },
      "outputs": [],
      "source": [
        "device = 'cuda'\n",
        "clip_model, preprocess = clip.load(\"ViT-L/14@336px\", device=device, jit=False)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('sberbank-ai/rugpt3small_based_on_gpt2')\n",
        "prefix_length= 50\n",
        "model_path = 'prefix_1-007.pt'\n",
        "model = ClipCaptionModel(backbone = 'gpt2', prefix_length = 50)\n",
        "model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
        "model.to(device)\n",
        "None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnGs2uMtH3IW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cl1Kh03qhq3-"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "import PIL\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import transformers\n",
        "import more_itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "#from tqdm import tqdm\n",
        "from dataclasses import dataclass, field\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "import cv2\n",
        "from PIL import Image\n",
        "def image_grid(imgs, rows, cols):\n",
        "    pils = imgs\n",
        "\n",
        "    assert len(imgs) == rows*cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid\n",
        "def read_video(path, transform=None, frames_num=9, window=30):\n",
        "    frames = []\n",
        "    cap = cv2.VideoCapture(path)\n",
        "\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    N = length//(frames_num)\n",
        "    #print(length)\n",
        "    #counter =\n",
        "\n",
        "    current_frame = 1\n",
        "    for i in range(length):\n",
        "\n",
        "        #frameId = int(round(cap.get(current_frame)))\n",
        "        #print(current_frame)\n",
        "        ret, frame = cap.read(current_frame)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        if ret and i==current_frame and len(frames)<frames_num:\n",
        "            size = 193, 193\n",
        "            frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "            frame.thumbnail(size, Image.ANTIALIAS)\n",
        "\n",
        "            frames.append(frame)\n",
        "            current_frame += N\n",
        "\n",
        "\n",
        "        #print(current_frame)\n",
        "        #cap.set(cv2.CAP_PROP_POS_FRAMES, current_frame)\n",
        "\n",
        "\n",
        "    cap.release()\n",
        "    #print(frames)\n",
        "    return frames\n",
        "\n",
        "\n",
        "\n",
        "def filter_ngrams(output_text):\n",
        "    a_pos = output_text.find(' A:')\n",
        "    sec_a_pos = output_text.find(' A:', a_pos + 1)\n",
        "\n",
        "    return output_text[:sec_a_pos]\n",
        "\n",
        "def generate2(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        tokens=None,\n",
        "        prompt='',\n",
        "        embed=None,\n",
        "        entry_count=1,\n",
        "        entry_length=67,  # maximum number of words\n",
        "        top_p=0.98,\n",
        "        temperature=1.,\n",
        "        stop_token = '.',\n",
        "):\n",
        "    model.eval()\n",
        "    generated_num = 0\n",
        "    generated_list = []\n",
        "    stop_token_index = tokenizer.encode(stop_token)[0]\n",
        "    filter_value = -float(\"Inf\")\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for entry_idx in range(entry_count):\n",
        "            if not tokens:\n",
        "                    tokens = torch.tensor(tokenizer.encode(prompt))\n",
        "                    #print('tokens',tokens)\n",
        "                    tokens = tokens.unsqueeze(0).to(device)\n",
        "\n",
        "            emb_tokens = model.gpt.transformer.wte(tokens)\n",
        "\n",
        "            if embed is not None:\n",
        "                generated = torch.cat((embed, emb_tokens), dim=1)\n",
        "            else:\n",
        "                generated = emb_tokens\n",
        "\n",
        "            for i in range(entry_length):\n",
        "\n",
        "                outputs = model.gpt(inputs_embeds=generated)\n",
        "                logits = outputs.logits\n",
        "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "                cumulative_probs = torch.cumsum(nnf.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
        "                                                    ..., :-1\n",
        "                                                    ].clone()\n",
        "                sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "                logits[:, indices_to_remove] = filter_value\n",
        "                #\n",
        "                top_k = 2000\n",
        "                top_p = 0.98\n",
        "                #print(logits)\n",
        "                #next_token = transformers.top_k_top_p_filtering(logits.to(torch.int64).unsqueeze(0), top_k=top_k, top_p=top_p)\n",
        "                next_token = torch.argmax(logits, -1).unsqueeze(0)\n",
        "                next_token_embed = model.gpt.transformer.wte(next_token)\n",
        "\n",
        "                if tokens is None:\n",
        "                    tokens = next_token\n",
        "                else:\n",
        "                    tokens = torch.cat((tokens, next_token), dim=1)\n",
        "                generated = torch.cat((generated, next_token_embed), dim=1)\n",
        "\n",
        "                if stop_token_index == next_token.item():\n",
        "                    break\n",
        "\n",
        "            output_list = list(tokens.squeeze().cpu().numpy())\n",
        "\n",
        "            output_text = tokenizer.decode(output_list)\n",
        "            output_text = filter_ngrams(output_text)\n",
        "            generated_list.append(output_text)\n",
        "\n",
        "    return generated_list[0]\n",
        "#from tqdm import tqdm, trange\n",
        "\n",
        "\n",
        "def _to_caption(pil_image,prompt=''):\n",
        "    device = 'cuda:0'\n",
        "    image = preprocess(pil_image).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "\n",
        "        prefix = clip_model.encode_image(image).to(device, dtype=torch.float32)\n",
        "        prefix_embed = model.clip_project(prefix).reshape(1, prefix_length, -1)\n",
        "        if prompt:\n",
        "            generated_text_prefix = generate2(model, tokenizer, prompt=prompt, embed=prefix_embed)\n",
        "        else:\n",
        "            generated_text_prefix = generate2(model, tokenizer, embed=prefix_embed)\n",
        "    return generated_text_prefix.replace('\\n',' ').replace('\\xa0','')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijbsQSPJFINb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "668fa2e6d525479a9f7f97e7ab5983a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a155c8076f104e74a5f1207979833a3d",
              "IPY_MODEL_fdd89c6d9e0d439d83d0da15814ba391",
              "IPY_MODEL_51c2c394c46847d188aa7f01d7553184"
            ],
            "layout": "IPY_MODEL_11f743891d714b5ea200d3265857aca6"
          }
        },
        "a155c8076f104e74a5f1207979833a3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9482fe805bc740e787e776a65c0066c6",
            "placeholder": "​",
            "style": "IPY_MODEL_9452a4f0b23c4028bfa1156072239c3e",
            "value": "  0%"
          }
        },
        "fdd89c6d9e0d439d83d0da15814ba391": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca3d539a70364596abf667c4465a255c",
            "max": 18993,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_83d31aba6c604f1a996ccaa9bcfd8f08",
            "value": 11
          }
        },
        "51c2c394c46847d188aa7f01d7553184": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58d946ec8a0e4b5197aff33bbb650bac",
            "placeholder": "​",
            "style": "IPY_MODEL_73cd0a2d07aa49869e2a10e46b2e72e0",
            "value": " 11/18993 [00:12&lt;2:26:41,  2.16it/s]"
          }
        },
        "11f743891d714b5ea200d3265857aca6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9482fe805bc740e787e776a65c0066c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9452a4f0b23c4028bfa1156072239c3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca3d539a70364596abf667c4465a255c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83d31aba6c604f1a996ccaa9bcfd8f08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "58d946ec8a0e4b5197aff33bbb650bac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73cd0a2d07aa49869e2a10e46b2e72e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}